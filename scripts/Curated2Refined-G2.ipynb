{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.0 \nCurrent idle_timeout is 2880 minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 5\nSetting new number of workers to: 5\nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::114652167878:role/AWSGlueAndS3RoleGrupo2\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: f4205a84-0dec-45b5-a697-32989117c4b6\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.0\n--enable-glue-datacatalog true\nWaiting for session f4205a84-0dec-45b5-a697-32989117c4b6 to get into ready status...\nSession f4205a84-0dec-45b5-a697-32989117c4b6 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Loading DataFrames with schema",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n\nusers_schema = StructType() \\\n      .add(\"#id\",StringType(),True) \\\n      .add(\"country\",StringType(),True) \\\n      .add(\"registered\",StringType(),True) \\\n      .add(\"date\",IntegerType(),True)\n      \nhoroscope_schema = StructType() \\\n      .add(\"Horoscope\",StringType(),True) \\\n      .add(\"TimeStart\",StringType(),True) \\\n      .add(\"TimeEnd\",StringType(),True) \\\n      .add(\"TimeStartInt\",IntegerType(),True) \\\n      .add(\"TimeEndInt\",IntegerType(),True)\n\n\nusersSource = \"s3://pfinal-p2-grupo2/curated/userid-profile.csv\"\nhoroscopeSource = \"s3://pfinal-p2-grupo2/curated/horoscopeCured.csv\"\ncountriesSource = \"s3://pfinal-p2-grupo2/curated/countryContinentCured.csv\"\n\ndf_users = spark.read.option(\"header\", \"true\").schema(users_schema).csv(usersSource)\ndf_horoscope = spark.read.option(\"header\", \"true\").schema(horoscope_schema).csv(horoscopeSource)\ndf_countries = spark.read.option(\"header\", \"true\").csv(countriesSource)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- #id: string (nullable = true)\n |-- country: string (nullable = true)\n |-- registered: string (nullable = true)\n |-- date: integer (nullable = true)\n\nroot\n |-- Horoscope: string (nullable = true)\n |-- TimeStart: string (nullable = true)\n |-- TimeEnd: string (nullable = true)\n |-- TimeStartInt: integer (nullable = true)\n |-- TimeEndInt: integer (nullable = true)\n\nroot\n |-- country: string (nullable = true)\n |-- continent: string (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Users DataFrame join with Countries DataFrame by country\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "df_user_countries = df_users.join(df_countries, df_users.country == df_countries.country, 'left').drop(df_users.country)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "907\n908\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Users_Countries DataFrame join with Horoscope DataFrame by date range\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import functions as F\ndf_hor_users_country = df_user_countries.join(df_horoscope, F.col(\"date\").between(F.col('TimeStartInt'), F.col('TimeEndInt')), 'left').drop(\"TimeStart\", \"TimeEnd\", \"TimeStartInt\", \"TimeEndInt\", \"date\")",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------+----------+------------------+---------+---------+\n|        #id|registered|           country|continent|Horoscope|\n+-----------+----------+------------------+---------+---------+\n|user_000533|2009-11-05|             Japan|     Asia|    Libra|\n|user_000497|2008-02-16|     United States| Americas|Capricorn|\n|user_000872|2008-01-31|            Brazil| Americas|Capricorn|\n|user_000651|2007-12-17|     United States| Americas|  Scorpio|\n|user_000650|2007-12-16|     United States| Americas|  Scorpio|\n|user_000435|2007-10-18|            Poland|   Europe|    Virgo|\n|user_000775|2007-10-09|           Finland|   Europe|    Virgo|\n|user_000956|2007-09-25|            Poland|   Europe|    Virgo|\n|user_000579|2007-09-24|    United Kingdom|   Europe|    Virgo|\n|user_000500|2007-09-23|     United States| Americas|    Virgo|\n|user_000218|2007-09-19|       Switzerland|   Europe|      Leo|\n|user_000786|2007-09-17|Russian Federation|   Europe|      Leo|\n|user_000246|2007-09-02|    United Kingdom|   Europe|      Leo|\n|user_000017|2007-08-27|           Morocco|   Africa|      Leo|\n|user_000725|2007-08-21|            Turkey|     Asia|   Cancer|\n|user_000652|2007-08-18|           Germany|   Europe|   Cancer|\n|user_000953|2007-08-15|    United Kingdom|   Europe|   Cancer|\n|user_000627|2007-08-13|Russian Federation|   Europe|   Cancer|\n|user_000037|2007-08-13|     United States| Americas|   Cancer|\n|user_000660|2007-08-12|           Croatia|   Europe|   Cancer|\n+-----------+----------+------------------+---------+---------+\nonly showing top 20 rows\n\n908\n908\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Write DataFrame as parquet file partitioned by country\n",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df_hor_users_country.write.partitionBy(\"country\").parquet(\"s3://pfinal-p2-grupo2/refined/TablaMaestra/tablamaestra.parquet\")\n\ndf_hor_users_country.write.csv(\"s3://pfinal-p2-grupo2/csv/TablaMaestra\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}